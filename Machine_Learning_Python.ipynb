{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4974ac9",
   "metadata": {},
   "source": [
    "# Machine Learning using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b37d8",
   "metadata": {},
   "source": [
    "## Machine Learning Life Cycle\n",
    "    #Predictive Modelling\n",
    "    #Predicting the future data based on past data(historical facts)\n",
    "    #Problem Def --> Hypothesis Gen --> Data extract --> Data Exploration --> Predictive Modelling --> Model Deployment/Implement\n",
    "    #Problem Definition: define problem so that it can be measured and clearly defined\n",
    "    #Hypothesis Generation: List down all posssible variables, which might influence problem objective\n",
    "    #Data Extraction: Primary and Secondary data collection sources\n",
    "    #Data Exploration: Understand the data, trends in data,cleansing and transforming data for analysis\n",
    "    # --> Univariate Analysis --> Bivariate Analysis --> Missing Value Treatment --> Outlier Treatment --> Variable                       Transformation\n",
    "    #Predictive Modelling: Process to create a statistical model for estimating/predicting the future results based on past          data\n",
    "    #Basically the Probability that needs to be calculated.\n",
    "        #Steps in PM: Algorithm selection --> Training --> Predictions\n",
    "        #Algorithm Selection: Selecting Dependent/Independent variables...based on that Decide(Supervised/UnSupervised                  algorithm)\n",
    "    #Supervised based on Dependent variable and Un-Supervised without an Dependent variable\n",
    "      #SuperVised Learning: If Dependent variable is continuous then go for Regression algorithms(Simple/Multiple Regression)\n",
    "      #SuperVised Learning: If Dependent variable is categorical then go for Classification algorithms(Logistic/Decision              Tree/Random Forest)\n",
    "      #UnSupervised learning: In this case Clustering algorithms be used(Mainly inference is the purpose)\n",
    "        \n",
    "    #Training Model: Process to learn relationship/correlation between independent and dependent variables\n",
    "        #Dataset used for training the model to understand the outcomes based on the value of dependent variable\n",
    "        #Test Dataset is used to validate the model using the data where dependent variable is not known\n",
    "        #Model Deployment: Implement the Model in real world in product(Ecommerce applications,Anti-virus,Spam Filters,Mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab0438f",
   "metadata": {},
   "source": [
    "## Problem Definition: \n",
    "    #Underlying causes,Why now/why requirements now, Is there time senstivity, Any timeline before this to be done\n",
    "    #Owner of the problem\n",
    "    #Measuring the problem solution(Metrics to measure success/failure, Benefits of doing such problem analysis)\n",
    "    #Also Constraints: Tradeoffs, Challenges\n",
    "    #Stakeholders: Directions, Marketing Head and Communications Team/Head\n",
    "    #References: Any past action on similar problems in the past and learn from it to proceed\n",
    "    #Example: What customer segments are more likely to churn balances in the next quarter by at least 50% considering current qtr?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cfc51",
   "metadata": {},
   "source": [
    "## Hypothesis Generation:\n",
    "    #View or assertion of a data scientist about the problem he or  she is working on.it may or may not be true.\n",
    "    #In case of Non-hypothesis format, it is difficult to analyze all the variables and arrive at an probable workflow and lots of bias\n",
    "    #But in hypothesis, listing out the assertion initially allows to work with smaller segment and provides an unbiased view\n",
    "    #Hypothesis should be done by clients,Data science team,marketing team and sales team so that there is a comphrensive analysis\n",
    "    #So the Purpose of Hypothesis is to list out all the featues on which our target variable might depend(without looking data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d65e92d",
   "metadata": {},
   "source": [
    "## Problem: What customer segments are more likely to churn balances in the next quarter by at least 50% considering current qtr?\n",
    "    #Start hypothesis with following\n",
    "    #Demographics(Gender,Age,Economical Status,Marital Status,Area,Education Levels,)\n",
    "    #Behavior (Vintage Customer,Spending behavior,Social media behavior)\n",
    "    #Psychographic(Travellers,Sports enthusiast,Movie goers.....)\n",
    "    #Other Factors: Competition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c6c2e",
   "metadata": {},
   "source": [
    "## Data Collection:\n",
    "    #Primary Data sources\n",
    "    #Customer Demographics data(Rural/Urban,Gender,Age....)\n",
    "    #Transactional Data(Assets/Libalities,Credits/Debits...)\n",
    "    #Individual data(Offline data)\n",
    "    #Secondary Data(Through Agents, Social Media,Spending Patterns from 3rd party sources...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e952a7",
   "metadata": {},
   "source": [
    "## Data Exploration: (Exploratory Data Analysis)\n",
    "    #EDA helps to get insights and inferences on the data(Nature of the data,Types of variables,Correlations,Data Consistency,Anamolies & Errors)\n",
    "    #EDA mainly works on Feature Engineering(Understand data and create new Features for further exploration)\n",
    "    #EDA is a five step process\n",
    "        #Variable Identification\n",
    "        #Univariate Analysis(for Numerical/Categorical variables)\n",
    "        #BiVariate Analysis(Relation between variables)\n",
    "        #Missing Values,Errors in DataSet\n",
    "        #Outliers/Anamolies to the analysed upfront\n",
    "    #Univariate Analysis(Analysing variables using Central Tendency,Range,Average/Mean,Median,Mode)\n",
    "    #BiVariate Analysis(Correlation/Covariance,Chi-Square Test for Similiraties/Differences in samples of data)\n",
    "    #Experiments: T-Test(One-Sample,Paired,two-Sample)\n",
    "    #Population Estimation: Central Limit Theorem\n",
    "\n",
    "    #Statistics(Types: Descriptive & Inferential)\n",
    "    #Descriptive: Descriptive Statistics deals with the summarisation of the data at hand.\n",
    "        #Basic data summary of an problem at hand(how many admissions to schools during the year from specific location, how many kgs of meat sold on weekdays/weekends)\n",
    "        #So the above problems could identify the information regarding the reasons/constraints\n",
    "        #This could be analyzed through visual reprsentations(Tables,Pie charts, Bar,Histograms....etc)\n",
    "\n",
    "    #Inferential: Estimated Analysis based on the given data\n",
    "    #Approximation of Population estimate from sample data\n",
    "    #Calculate/Estimate metrics based on the given sample and approximate for the population at large\n",
    "    #Basically does(Approximation based on Sample,Metrics & Error Analysis of the Sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760aba7",
   "metadata": {},
   "source": [
    "## Building the Predictive Modelling\n",
    "    #First Estimate --> Then Base Case Model(Avg/Mean of the score prepared by offline) --> Predictive model should better the base model\n",
    "    #How to Generate Base case Model:\n",
    "        #1.Create the Dataset for Predictive Model\n",
    "        #2.Methods to Generate Predictions\n",
    "        #3.Generate Predictions\n",
    "        #4.Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets take a big mart sales data and predict the sales on a particular outlet\n",
    "#Mean of data, Mean Absolute error\n",
    "#Mean Absolute Error(Sum of differences between every observation divided by total number of observations)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv('datasets/train_bm.csv')\n",
    "data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "338957fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1674443637.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['simple_mean'] = train['Item_Outlet_Sales'].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1355.4481105570344"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Work on the Train data of Big mart sales above\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data,random_state=42)\n",
    "#creating 4 divisions\n",
    "div = int(data.shape[0]/4)\n",
    "train = data.loc[:3*div+1:]\n",
    "test = data.loc[3*div+1:]\n",
    "train\n",
    "\n",
    "#Base Mean model\n",
    "test['simple_mean'] = train['Item_Outlet_Sales'].mean()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "test\n",
    "simple_mean_error = mae(test['Item_Outlet_Sales'],test['simple_mean'])\n",
    "simple_mean_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d062316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'] = 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\4155826734.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1138.8026221064356"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean by outlet type\n",
    "out_type = pd.pivot_table(train,values='Item_Outlet_Sales',index=['Outlet_Type'],aggfunc=np.mean)\n",
    "out_type\n",
    "test['Out_type_mean'] = 0\n",
    "for i in train['Outlet_Type'].unique():\n",
    "    # Assign the mean value corresponding to unique entry\n",
    "    test['Out_type_mean'][test['Outlet_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Type'] == str(i)].mean()\n",
    "\n",
    "#calculating mean absolute error\n",
    "out_loc_error = mae(test['Item_Outlet_Sales'] , test['Out_type_mean'] )\n",
    "out_loc_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac70fe8",
   "metadata": {},
   "source": [
    "## Mean Item Outlet Sales with respect to Outlet_Establishment_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3032b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'] = 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\2419951118.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1267.26335489281"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_year = pd.pivot_table(train, values='Item_Outlet_Sales', index = ['Outlet_Establishment_Year'], aggfunc=np.mean)\n",
    "out_year\n",
    "\n",
    "# initializing new column to zero\n",
    "test['Out_year_mean'] = 0\n",
    "\n",
    "# For every unique entry in Outlet_Identifier\n",
    "for i in train['Outlet_Establishment_Year'].unique():\n",
    "  # Assign the mean value corresponding to unique entry\n",
    "  test['Out_year_mean'][test['Outlet_Establishment_Year'] == i] = train['Item_Outlet_Sales'][train['Outlet_Establishment_Year'] == i].mean()\n",
    "    \n",
    "    \n",
    "#calculating mean absolute error\n",
    "out_year_error = mae(test['Item_Outlet_Sales'] , test['Out_year_mean'] )\n",
    "out_year_error    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe22be3",
   "metadata": {},
   "source": [
    "## Mean Item Outlet Sales with respect to Outlet_Location_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56c07096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1557353145.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['out_loc_mean'] = 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1557353145.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['out_loc_mean'][test['Outlet_Location_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Location_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1557353145.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['out_loc_mean'][test['Outlet_Location_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Location_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1557353145.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['out_loc_mean'][test['Outlet_Location_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Location_Type'] == str(i)].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\1557353145.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['out_loc_mean'][test['Outlet_Location_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Location_Type'] == str(i)].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1348.8509267072286"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_loc = pd.pivot_table(train, values = 'Item_Outlet_Sales', index = ['Outlet_Location_Type'], aggfunc = np.mean)\n",
    "out_loc\n",
    "\n",
    "# Initializing empty column\n",
    "test['out_loc_mean'] = 0\n",
    "\n",
    "# For every unique entry in Item_Identifier\n",
    "for i in train['Outlet_Location_Type'].unique():\n",
    "  # calculate and assign mean corresponding to the uniques entries\n",
    "  test['out_loc_mean'][test['Outlet_Location_Type'] == str(i)] = train['Item_Outlet_Sales'][train['Outlet_Location_Type'] == str(i)].mean()\n",
    "    \n",
    "#calculating mean absolute error\n",
    "out_loc_error = mae(test['Item_Outlet_Sales'] , test['out_loc_mean'] )\n",
    "out_loc_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69579f1",
   "metadata": {},
   "source": [
    "## Mean Item_Outlet_Sales with respect to both Outlet_Location_Type and Outlet_Establishment_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b531d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'] = 0\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1936\\3503836485.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1140.0522313200124"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo = pd.pivot_table(train, values = 'Item_Outlet_Sales', index = ['Outlet_Location_Type','Outlet_Establishment_Year'], aggfunc = np.mean)\n",
    "combo\n",
    "\n",
    "# Initiating new empty column\n",
    "test['Super_mean'] = 0\n",
    "\n",
    "# Assigning variables to strings ( to shorten code length)\n",
    "s2 = 'Outlet_Location_Type'\n",
    "s1 = 'Outlet_Establishment_Year'\n",
    "\n",
    "# For every Unique Value in s1\n",
    "for i in test[s1].unique():\n",
    "  # For every Unique Value in s2\n",
    "  for j in test[s2].unique():\n",
    "    # Calculate and Assign mean to new column, corresponding to both unique values of s1 and s2 simultaneously\n",
    "    test['Super_mean'][(test[s1] == i) & (test[s2]==str(j))] = train['Item_Outlet_Sales'][(train[s1] == i) & (train[s2]==str(j))].mean()\n",
    "    \n",
    "#calculating mean absolute error\n",
    "super_mean_error = mae(test['Item_Outlet_Sales'] , test['Super_mean'] )\n",
    "super_mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d4f82",
   "metadata": {},
   "source": [
    "## Classification problems: Purpose is to classify data in to groups and predict the outcomes accordingly\n",
    "    #Evaluation of the model using the Model Accuracy(Accurate Predictions/Total Predictions)\n",
    "    #Confusion Matrix:Predicted Outcome Vs Actual value(True Positives,True Negatives,False Positives,False Negatives)\n",
    "    #Correct Prediction is (True Positives + True Negatives)/ Total Predictions...That's the Accuracy part\n",
    "\n",
    "    #Confusion Matrix\n",
    "                                            \n",
    "                                               #Predicted\n",
    "                            #Positive                        #Negative\n",
    "                ####################################################################\n",
    "                #                              #                                   #\n",
    "    #Positive   #                              #                                   #\n",
    "                #         True Positive        #            False Negatives        #\n",
    "                #                              #                                   #\n",
    "#Actuals        #                              #                                   #           \n",
    "                ####################################################################\n",
    "                #                              #                                   #\n",
    "    #Negative   #         False Positive       #            True Negative          #                                                                 \n",
    "                #                              #                                   #\n",
    "                #                              #                                   #  \n",
    "                ###################################################################\n",
    "                \n",
    "    #TPR(TP/TP + FN) ---True Positive Rate --- True Predicted positive / Total Actual Positives(Indicate how accurate the True positives are )\n",
    "    #FNR(FN/TP + FN) ---False Negative Rate --- False Negatives Predicted/ Total Positives( Indicates how the model has wrongly predicted Negatvies which were positives/out of total actual positive)\n",
    "    # FNR...Lower FNR value denotes model is better...since there is lesser observations predicted wrongly on positives\n",
    "    #TNR(TN/FP + TN)--- True Negative Rate --- True Negative Prediction vs Total Actual Negatives...(Indicates what percentage of Negative Values the Model is able to predict)\n",
    "    #FPR(FP/FP+TN)---- False Positive Rate ---> Actual Negative Predicted as Positives vs Total Actual Negatives(Indicates what percentage of Positives wrongly predicted )\n",
    "    #So better Model signifies....(Higher TPR, Lower FNR, Higher TPR, Lower FPR tending to 0 )\n",
    "\n",
    "    #Precision & Recall\n",
    "        #Precision = TP/TP + FP (Predictions which are positive vs Total Predictive Positives)\n",
    "            #Evaluation Metrics:\n",
    "            #Minimize False Positives\n",
    "            #False Negative Rate is high\n",
    "        #Recall: Predicted Actual Positive/Total Actual Positives() = TP/TP+FN\n",
    "            #Recall is used when we cannot afford False Positives or Avoiding False Negative i Prioritized over encountering False Positives\n",
    "        #Always Tradeoff beween Precision and Recall\n",
    "        #High Precision, Low Recall or High Recall, Lower Precision...(Depending on the use-case problem either option to be selected)\n",
    "        #So F1 Score is used to combine both the Precision & Recall Metric()\n",
    "        #So F1 score is maximum when Precision = Recall\n",
    "        #Good F1 score indicates a better model.\n",
    "        #TPR = 1 - FPR & FPR = 1-TPR\n",
    "        #FNR = 1 - TNR & TNR = 1-FNR\n",
    "\n",
    "    #Threshold values\n",
    "        #Useful to categorize the Prediction probabilities(we set a threshold value of 0.5 and then find the performance of the Model in terms of it's accuracy)\n",
    "        #Evaluate model using AUC-ROC\n",
    "        #ROC(Receiver Operating Characteristic) curve...It Plots 2 parameters True Positive Rate(TPR) & False Positive Rate(FPR)\n",
    "        #ROC is used as Evaluation Metric for binary classification(originally used for distinguishing noise and not noise)\n",
    "        #AUC -- Area Under the Curve(That means...it is an sorting based algorithm that can provide the points in the ROC curve\n",
    "        #AUC -- Provides an aggregate measure of performance across all possible classification thresholds.\n",
    "        #AUC -- AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n",
    "        #AUC is desirable for the following two reasons:\n",
    "            #AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "            #AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen.\n",
    "            #AUC Roc cannot be used for comparing 2 models\n",
    "        #Log Loss: Negative average of the log of corrected predicted probabilities for each instance.\n",
    "        #Log(Corrected Probablities)\n",
    "            # Log Loss = 1/N {yi * log(pi) + (1-yi) * log(1-pi)}\n",
    "\n",
    "    #Evaluation Metrics: Regression\n",
    "        #MAE(Mean Absolute Error) Mean( Predicted - Actuals)\n",
    "        #MSE(Mean Squared Error) Square of the Mean(Predicted - Actuals)\n",
    "        #RMSE(Root Mean Squared Error) (Square Root of the Mean Squared Error of (Predicted-Actuals))\n",
    "        #RMSLE(Root Mean Squared Log Error)(Square of the Log of Mean Square of(Predicted-Actuals))\n",
    "        #R-SQUARED (Relative Squared ..Benchmarking against Avg Model...which is basically the mean of the Values of Y)\n",
    "        #R-SQUARED(On the addition of features in model, R-Squared either increases or remains the same.It never decreases)\n",
    "        #Adjusted R-Squared(Handle the Features in the model and based on that calculates the metric value)\n",
    "\n",
    "    #Pre-Processing Techniques before doing Model building\n",
    "        #Imputting Missing Values\n",
    "        #Remove Categorical(String) variables...so to handle numerical computations\n",
    "        #Treat Outliers\n",
    "        #Feature Scaling and Variable Transformation\n",
    "    #Reason for missing values:\n",
    "        #Human Error\n",
    "        #Extraction Error\n",
    "        #Customer's Privacy\n",
    "        #Other Factors\n",
    "        #Normally Missing values are represented as 'NaN'/'NA'/'Unknown'/'missing'\n",
    "    #Treating Missing values:\n",
    "        #Remove Data points having missing values in the dataset\n",
    "        #dropna function can be used in python to remove the missing values in any of the columns\n",
    "        #problem with dropna is it can remove lot of data points and it is not good for model building as information is lost\n",
    "        #Removing Columns with Missing Values( Need to take in to account the % of rows where missing values is present in the          column)\n",
    "        #dropna with threshold value will be verified for deleting a column(threshold of 500 means delete if any row count have          >= 500)\n",
    "        #Replace with a new category/value\n",
    "            #Replace it with Extreme values(for ex for numerical column: 'unknown', 'missing'; Numeric:999)\n",
    "            #Create a separate feature(to denote the missing column values)\n",
    "            #In python we can use fillna function to all the missing values\n",
    "        #Replace with using Central Tendency(Categorical column using Mode of the column;Numerical column using Mean/Median              values)\n",
    "        #Replace with using the Relationship with features\n",
    "            #using another variable with high correlation\n",
    "            #Patterns among features\n",
    "            #using ML Model(KNN can be used for the same)\n",
    "            #Using the feature of an ML model which is trained.\n",
    "\n",
    "    #Pre-Processing categorical variables\n",
    "        #Categorical variable(Nominal/Ordinal types)\n",
    "        #Nominal(Without order or comparisons...Gender,Occupation & City Names)\n",
    "        #Ordinal(Order based...Education,Remarks,Moving Ratings)\n",
    "        #Label Encoding is way to handle the Categorical numericals to conver to Numerical values\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cce70e",
   "metadata": {},
   "source": [
    "## Selecting the Right Model\n",
    "    #Underfitting:\n",
    "        #The Model has to perform good in training as well as test data.\n",
    "        #Undersfitting means training set is not enough for a better model performance during test evaluation\n",
    "    #Overfitting:\n",
    "        #Overfitting means training data set performs better but on the test data set the model evaluation is bad...hence\n",
    "    #Best Fit:\n",
    "        #Model performs equally well in train and test data set.\n",
    "\n",
    "    #Since the Model sees only the Train & Test Data to calculate the F1Score against K value(KNN Model)...We need to validate using\n",
    "    #different data set(thats the Validation Set which will validate the model performance before test does final evaluation)\n",
    "    #Hold-Out Validation:\n",
    "        #First distribute the data as major part to Training and remaining to test and validation set\n",
    "        #could be like 70:15:15\n",
    "        #But the Hold-out validation has problem if the Target Class is not distributed properly among train and validation set.\n",
    "        #To Resolve that we need to go for Stratified Hold-out validation\n",
    "    #Stratified Hold-Out Validation:\n",
    "        # For unbalanced dataset , we use stratified option for hold-out validation to distribute the classes equally on all the\n",
    "        # 3 data sets(train,test & validation sets)\n",
    "\n",
    "    #K-Fold cross validation:(It is used to check the consistency of the model)\n",
    "        #Shuffle the Dataset randomly\n",
    "        #Split the dataset into k groups\n",
    "            #pick a group as hold out\n",
    "            #Take the remaining groups as training and fit a model\n",
    "            #predict and evaluate a model\n",
    "            #Basically does multiple validation sets...since it is having multiple groups\n",
    "            #How to decide the value of K? (Good value of around 5 < K < 10)\n",
    "                #Validation size = statistically significant for a better model\n",
    "                #In case of lower/Higher K values...then there is chance of higher bias\n",
    "            #Leave one out Cross Validation: For smaller datasets( It is an extreme case of K-fold cross validation)\n",
    "                #N instances, N Models\n",
    "                #Not significant when dataset is large\n",
    "                #Best practices for smaller datasets\n",
    "\n",
    "    #Bias and Variance\n",
    "    ![Bias_Variance.png](attachment:Bias_Variance.png) (Change to Markdown to see the image)\n",
    "    #Higher F1 score denotes better model\n",
    "    #Model exhibits high bias if it could not learn certain patterns in data\n",
    "    #When model captures noise(overfitting data) instead of only signals then there is higher chances of bias in the model\n",
    "    #Best fit is the model which has actually reduces bias and variance in the system\n",
    "    # Error type: Higher/Lower Bias/Higher Variance/Low variance\n",
    "    # Fit: Underfit/Overfit/Best fit\n",
    "    # K-Range: value of K to arrive at the optimcal model performance\n",
    "    # Complexity: Low/High Complexity\n",
    "    \n",
    "## Bias vs Variance\n",
    "    #High Bias observed when model is very simple ...when high values of K...with Low Variance\n",
    "    #Low Bias observed when model is very complex...for low values of K. with high variance\n",
    "    #So As bias increases variance decreases and as bias decreases variance increas.\n",
    "    #So Best fit model has both Variance and Bias as lower values.\n",
    "    #High Bias --> Underfit model\n",
    "    #Low Bias --> Overfit model\n",
    "    #Optimal Bias --> best fit model\n",
    "    #Bias is the amount that a model’s prediction differs from the target value, compared to the training data. Bias error results from simplifying the assumptions used in a model so the target functions are easier to approximate. Bias can be introduced by model selection. Data scientists conduct resampling to repeat the model building process and derive the average of prediction values. Resampling data is the process of extracting new samples from a data set in order to get more accurate results. There are a variety of ways to resample data including:\n",
    "        #K fold resampling, in which a given data set is split into a K number of sections, or folds, where each fold is \n",
    "        #used as a testing set.\n",
    "        #Bootstrapping, which involves iteratively resampling a dataset with replacement.\n",
    "        \n",
    "    #Variance indicates how much the estimate of the target function will alter if different training data were used.\n",
    "        #In other words, variance describes how much a random variable differs from its expected value. \n",
    "        #Variance is based on a single training set. Variance measures the inconsistency of different predictions \n",
    "        #using different training sets — it’s not a measure of overall accuracy.\n",
    "        #Variance can lead to overfitting, in which small fluctuations in the training set are magnified. \n",
    "        #A model with high-level variance may reflect random noise in the training data set instead of the target function. \n",
    "        #The model should be able to identify the underlying connections between the input data and variables of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25731aaa",
   "metadata": {},
   "source": [
    "## Linear Models( for Linear Relationship features)\n",
    "    #When there is a linear relationship between Depedent and Independent variable\n",
    "    #Increase/decrease in one variable leads to increase/decreases of other variable\n",
    "    #Price of Land based on Size of Land\n",
    "    #Equation: Y = |B(X) + b where ...|B denotes where the line cuts the horizontal axis...b refers the y-intercept/distance of      the line from (0,0)\n",
    "    # Line which give Least MSE....then it is best line fit that can be used to do prediction.\n",
    "    #Lower the MSE = Better Model\n",
    "    #|B denotes the Slope.....and b reprsents the intercept where it hits the Y-axis.\n",
    "    #Compute Error(Cost associated)\n",
    "\n",
    "    #Calculate Beta(|B) & Y-Intercept(b):\n",
    "        # Cost Function: Used to calcalate Errors for the Lines plotted for Linear Relationship\n",
    "        # Various values of Beta(|B) and MSE(Mean Squared Error)\n",
    "        # Plot the cost curve and find out the error is minimum for different Beta values selected\n",
    "\n",
    "    #Intuition to Gradient Descent(To calculate Beta and Intercepts)....Need to study more\n",
    "        #Cost/Error corrections to arrive at the optimal solution\n",
    "            #Random Initiation,Generate Predictions,Calculate Cost,Update Parameters\n",
    "            #Gradient is basically used to optimize the algorithm\n",
    "            #Basically optimization is done by minimizing the error generated iteratively\n",
    "        #Math behind Gradient Descent:\n",
    "            # Gradient in Linear Regression: To Findout the values of Beta and Intercept\n",
    "                #Start as benchmark model with Beat = 0 and Intercept = Mean of Dependent variable\n",
    "                # Y = B as initial eqn for the predictions\n",
    "                #Calculate cost(MSE) ....based on Prediction and Actuals\n",
    "                #If Cost goes zero that means the model is improving....for other cases it needs iterations to improve the cost\n",
    "                #Partial derivatives give slope of the point or the MSE eqn.\n",
    "                # Beta = Beta - Alpha(GBeta)....alpha is the step size to reach the minima\n",
    "                # So basically tweak the parameters until your cost comes down and fit the best linear regression line.\n",
    "        #Convexity of Cost Function:\n",
    "            #Shape of the Cost function is Convex curve...so one single minima is possible\n",
    "            #In case of Multiple Minima...how to avoid local minima and go for global minima and get the best fit line\n",
    "            #How to do that?\n",
    "                #Random Initialization(Keep tweaking the starting set of parameters)\n",
    "                #Adjust Learning Rate (Used for converging the global minima)\n",
    "        \n",
    "        #Assumptions of Linear Regression:\n",
    "        #Linear Relationships should be there for Dependent vs Independent features...If not then we need to do some Log/X-              Squared transformtion accordingly\n",
    "        #Correlation between Error Terms:Residuals which is plotted should not show a trend and fairly it should be random\n",
    "        #residual is basically the difference between Actuals vs the Predicted Line\n",
    "        #Constant variance of Error: If there is a trend in variance of errors, then it is not good for linear regression...so          need to be constant variance\n",
    "        #MultiCollinearity: Interrelation between 2 variables will cause the equation a lot and will be difficult to interpret          the model prediction\n",
    "        #If 2 variables are highly multicollinear, we can eliminate one of the variables and how to do that?\n",
    "            #Vif\n",
    "                #(Used for Diagonising collinearity/multicollinearity)\n",
    "                #Quantifies correlation b/w one and other predictor\n",
    "                #Take independent variables and fit a model by taking out one as dependent and other independent variables\n",
    "                 # So if the model predicts one of the dependent variable taken we will have high R2\n",
    "                    #VIF(Variance Interval Frequency) = 1/1-R2...so if R becomes 1...then ViF becomes huge...thats not good.\n",
    "                    #If VIF > 4 need to be removed and hence that supports MultiCollinearity\n",
    "                    #R2 = 1 - MSE(Model)/MSE(baseLine)\n",
    "                    #Final assumption....errors to be normally distributed...Plot the residuals and check for normal distribution\n",
    "                    #Within the Quantitles the residuals to be normally distributed.\n",
    "                    #In case of Non-Normal distribution, we need to perform transformations on data and do the prediction again.\n",
    "            \n",
    "\n",
    "    #Generalized Linear Model(Y = Beta*F(X) + C)....here F(X) can be of any types...polynomial/quadratic/cube functions....or \n",
    "        # F(X) --> Generalized function which can take any values of X....C is a constant\n",
    "        #Logistic Regression(Classification algorithm)\n",
    "        #Log Loss formula to be used for cost function for Logistic regression\n",
    "        #Supervised Algorithm used for classification models\n",
    "        #In this case, a sigmoid function is used to transform the linear regression line to an sigmoid curve with limits from          0->1\n",
    "        #Mainly Recall evaluation metric to be used for Logistic Regression Model(Minimize the False negatives)\n",
    "        #Log of Odds: Odds = P/(1-P)....BetaX + b = log(P/(1-P))\n",
    "\n",
    "## Problems with Linear Regression:\n",
    "    # More Features participating, then high coefficients...that will lead to over fit....so it's not a Genaralized Linear Model\n",
    "    # that means it performs well on train but not on test dataset\n",
    "    # Regularization: Model captures the noise with the predictors.\n",
    "        #Ridge Regularization:\n",
    "            # J = Mean Squared Errors + Sum of Square of all Coefficients\n",
    "            # J = {Sqr(Yi^ - Yi)/n + Sum of Squares of all coefficients\n",
    "            # Reduces impact of Cofficients but not goes to zero\n",
    "            # Ridge still have more parameters...so difficult to interpret the model\n",
    "            # No reduction in the parameters.\n",
    "            \n",
    "        #Lasso Regularization:\n",
    "            # J = Mean Squared Error + Sum of Absolute value of all Coefficients\n",
    "            #Lasso gives less number of features\n",
    "            # This is useful when small subset of independent variables is significant in the predictive process.\n",
    "            # It does not reduce the features in the system for arriving at regularization.\n",
    "            \n",
    "        #Coefficient Estimate of Ridge & Lasso:\n",
    "        #Cost Function....{(Yi^ - Yi) ^2/n\n",
    "            #Gradient descent...point of high cost to low cost is the main reason.\n",
    "            #Shrinkage Parameters     \n",
    "## Excercises\n",
    "    #Implement( As Excercise)\n",
    "    #Linear Models & Generalized Linear Model\n",
    "    #Logistic Regression\n",
    "    #Logistic Regression(Churn Prediction...Customer will follow below the default balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffc172",
   "metadata": {},
   "source": [
    "# Decision Tree(Tree Based Algorithm)\n",
    "    #What is Decision Tree, Terminologies\n",
    "    #Splitting criteria in DT\n",
    "    #Pros & Cons of DT\n",
    "    #Implement the Decision Tree\n",
    "    \n",
    "    #What is Decision Tree:\n",
    "        # Non Parametric Algorithm rather Linear Regression is parametric based.\n",
    "        # Splitting the Data set based on criteria, so that to arrive at the best possible way to predict the outcome\n",
    "        # Tree Based algorithm where based on criteria we split it in to subsets and repeats until we find the leaf node...which\n",
    "        # gives our predictable outcome.\n",
    "        #Steps in DT Algorithm:\n",
    "            #Collect and preprocess the data\n",
    "            #|\n",
    "            #|\n",
    "            #Define the target variable and the input variables\n",
    "            #|\n",
    "            #|\n",
    "            #Split the data into training and test sets\n",
    "            #|\n",
    "            #|\n",
    "            #Choose the criteria for splitting the data\n",
    "            #|\n",
    "            #|\n",
    "            #Build the decision tree by applying the splitting criteria to the training data\n",
    "            #|\n",
    "            #|\n",
    "            #Use the decision tree to make predictions on the test data\n",
    "            #|\n",
    "            #|\n",
    "            #Evaluate the performance of the decision tree by comparing the predictions with the true values of the target variable\n",
    "        \n",
    "# Measuring Purity of the Node:\n",
    "    \n",
    "    #Gini Impurity(Binary Splits)\n",
    "    #Gini Impurity  = 1 - Gini\n",
    "    #Gini ranges from 0 to 1....higher value denotes pure nodes.\n",
    "    #Best split is determined using the Gini impurity\n",
    "    #Lower Gini Impurity higher the homogeneity of nodes\n",
    "    #Works only with Categorical Targets\n",
    "    #Steps:\n",
    "        #Gini impurity is started from sub nodes\n",
    "        #Gini is calculated as sum of squares of probabilities for each class/category(Gini = p1^2 + p2^2 +.....Pn^2)\n",
    "        #Weighted Gini impurity is calculated next based on the number of samples in node/total samples in parent node.\n",
    "        #Weigh Gini Impurity formula is:\n",
    "            # (No of samples in node1/total samples in parent) * Gini impurity of node1 + ......(No of samples in nodeN/total samples in parent) * Gini impurity of nodeN\n",
    "            #Class with minimum Gini Impurity will be selected\n",
    "\n",
    "    #ChiSquare(Statistical difference between child nodes and parent nodes)\n",
    "        #Measured as sum of squared standardized differences between actual and expected frequencies of target variable for each node.\n",
    "            #chi-square = sqrt([Actual-Expected]^2/Expected)\n",
    "            #Basically to test the distribution of classes in subnodes w.r.t to parent node...if distributions are same...then it means\n",
    "            #we are not improving the purity of the nodes.\n",
    "            #If the Chi-Square value is high, that means distribution in child nodes are change and hence purity is improving\n",
    "            #Chi-Square can work only with categorical variables and not continous variables.\n",
    "            #Chi-Square can perform 2 or more splits\n",
    "            #Higher the Chi-Square value, higher the homogenity of the nodes.\n",
    "            #Total of all chi-squares(of all the different splits) and compare it with other criteria.\n",
    "                #Higher chi-square value is taken to determine the purity of the nodes...so distribution is better homogenous\n",
    "                \n",
    "    #Information Gain(Nodes which has more information to describe)\n",
    "        #More impure nodes need more information to describe....less information gain\n",
    "        #On the other hand pure nodes have little information to describe as split is clearer...so high information gain.\n",
    "        #Information Gain = 1 - Entropy\n",
    "        #Where Entropy  = -p1*logP1 - p2*logP2........-pn*logPn...(Where p1,p2...indicates the probability of classes)\n",
    "        #Low Entropy means more purity and high entropy means impure.\n",
    "        #Entropy works only with categorical variables\n",
    "        #Lesser the entropy means higher the homogenity of the nodes.\n",
    "        #Steps to calculate Entropy of split:\n",
    "            #Calculate the entropy of the parent node\n",
    "            #Calculate the entropy of the child nodes\n",
    "            #Calculate the weighted average entropy of the of the split.\n",
    "                #Formula = node samples/total parent samples * entropy of child node + node samples2/total parent samples * entropy of child nodes2 +.......\n",
    "            #Calculate information Gain for the Entropy value\n",
    "            #Compare the entropy and information gain for all the splits taken\n",
    "            #Information Gain while it higher and lesser entropy gives more homogenity of the nodes.\n",
    "            #Information Gain while it lower and higher entropy gives lesser homogenity of the nodes.\n",
    "    \n",
    "    #Reduction in Variance\n",
    "        #Continous Target variable(for Reduction in Variance)\n",
    "        # Variance = Sum([(X - Mu)^2]/n) where n-sample size, Mu - mean of the sample\n",
    "        # Lower variance tends to more clarity of the class distribution as opposed to higher variance.\n",
    "        \n",
    "## Optimizing performance of Decision Trees:\n",
    "    # Split the nodes based on the features but with a constraint(otherwise it will keep increasing and could do overfitting)\n",
    "    # Decision Tree could do well in training data but could perform bad on validation data...hence there is overfitting\n",
    "        # we can set the threshold on the depth of the tree(controlling growth of the tree...its an hard constraint...but its\n",
    "        # performance could fail in train data itself...so that's underfitting)\n",
    "    #Other Options:(Need to identify the tradeoffs for each of the options)\n",
    "        #Minimum samples for node split( So minimum node samples will allow for split....so this should be optimum)\n",
    "        #Minimum samples for Terminal Node...Too high value lead to underfitting\n",
    "        #Maximum depth of tree(To control the splits)\n",
    "        #Maximum number of Terminal Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be4c24",
   "metadata": {},
   "source": [
    "# Feature Engineering:\n",
    "    #Adding more features to the model to improve the performance.\n",
    "    #Making use of existing External/Internal data\n",
    "    #Technique generating new variables by using current variables is called Feature Engineering.\n",
    "    #So having good knowledge of the problem and it's domain helps to build the new features\n",
    "    #Feature pre-processing and Generating\n",
    "        #Feature Preprocessing implies updating or transforming existing features\n",
    "        #Feature Generation is creating new featurefrom the existing features\n",
    "    #Feature Pre-Processing\n",
    "        #Feature Transformation...Transform features based on mathematical operations\n",
    "        #Log/Square,Square Root,Reciprocal\n",
    "        #For ex: if target variable and features are not linear...here we need to transform it to Linear regression for better model\n",
    "        #For ex: In otther case ,skewed distribution due to outliers could be transformed to normal distribution for model performance\n",
    "        # for ex: In case of Right skewed we can take Log/ nth root transformation to convert it to normal distribution\n",
    "        # for ex: In case of Left Skewed we can take nth power/exponential to convert it to normal distributions.\n",
    "        # Feature Scaling:\n",
    "            #In case of the Features with different scales we need to scale up/down to bring all in same scaling.\n",
    "            #Min Max Scaling:\n",
    "                # Xi = Xi - min(X)/max(X) - min(X) and  denominator represents max and min values of X parameter\n",
    "                #SCales down the feature variable between 0 to 1\n",
    "                \n",
    "        #Standard Scaling:\n",
    "                #Determine the mean and SD of the column\n",
    "                #Next subtract each value from Mean and divide by Standard Deviation\n",
    "                    # x~ = x - x^/Stddev ; where x^ is the mean....and Stddev is the Standard deviation\n",
    "        # Feature Preprocessing: One Hot Encoding:\n",
    "            #Categorical to Numerical variables for processing\n",
    "            #For ex: {Medium,Small,Large} can be encoded as {Size_Medium(0,1),Size_Small(0 or 1), Size_Large(0,1)}\n",
    "            #So in case of doing this...we should not loose the order in the variables(ordinal variables)\n",
    "            #So we can replace it with Label Encoding and having only single column with order maintained\n",
    "            #For ex: {Medium_Small,Large} can be encode in same column as {2,1,3}...so here the order is mainatined\n",
    "            #Similarly in Label encoding it does using Alphabetical order....so instead we will go for map function in python\n",
    "                # variable.map({\"Mediume:2\",\"Small:1\",\"Large:3\"})...so this will make it consistent and not loose the order info.\n",
    "        #Feature PreProcessing: Combine Sparse Classes:\n",
    "            # To Reduce the number of categories(Combne the categories which are less frequency)\n",
    "            \n",
    "        #Binning:\n",
    "            # Binning is the process of classification based on criterian...So we can create as many bins based on our optimal\n",
    "            # performance.\n",
    "            # For ex: In case of Age column, we can have bins like <=35 as Bin1; > 35 as Bin2.\n",
    "                #So this can be repeated for other columns.\n",
    "            #So basically process of transforming numerical variables to categorical variables\n",
    "        #Feature Interaction:\n",
    "            #Capture new variable based on Interaction betweeen multiple variables\n",
    "            #Need Domain knowledge to perform this\n",
    "            \n",
    "        #Frequency Encoding:\n",
    "            #Its a Feature Generation method.\n",
    "            #Frequency of the categories available in a variable and normalize accordingly.\n",
    "            #Frequency count on a specific variable helps to model the parameters accordingly to get some more insights\n",
    "            #value_count helps us to arrive at the frequency.\n",
    "            #It is always done on the categorical variable.\n",
    "            \n",
    "        #Mean Encoding:\n",
    "            # It is basically done at the target variable...For ex: if sales is target variable....then for each category of \n",
    "            # product/item we can calculate the mean sales for each item type and get more insights.\n",
    "        \n",
    "        #DateTime Features:\n",
    "            #Feature Engineering based on DateTime value.\n",
    "            #For ex: Booking a hotel for a particular month...Booking Time...and also Price of Flights over a time\n",
    "            # Using Date, we can get the day of the month, weekday/weekend/holidays...based on days, price of tickets changes.\n",
    "            #Python Datetime can helps to extract various information and use it for the model parameters.\n",
    "            #to_datetime function can helps with giving the datetime column and the format to be changed.\n",
    "        #Automating the Feature Engineering:\n",
    "            #FeatureTools is an tool, that helps us to create/generate features(Open Source Library)\n",
    "                # 3 Components(Entities,Feature Primitives & Deep Feature Synthesis)\n",
    "                # Entity is data table or dataframe(Entity set is a collection of tables)\n",
    "                # Feature Primitives(Aggregations,Transformations are called Feature primitives)\n",
    "                # Deep Feature Synthesis(Allows multiple aggregation and transformation operations)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfda5e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'set' and 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6512\\4131934620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mset1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mset2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mset2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'set' and 'set'"
     ]
    }
   ],
   "source": [
    "print(2**3 + (5+6) ** (1+1))\n",
    "set1 = {1,3,5}\n",
    "set2 = {2,4,6}\n",
    "print(len(set1 + set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4e5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(5/2)\n",
    "print(5//2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
